{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup Google Colab by running this cell only once (ignore this if run locally) {display-mode: \"form\"}\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/epfl-exts/aiml2days.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"aiml2days/notebooks/data\" \"aiml2days/notebooks/data_prep_tools.py\" \"aiml2days/notebooks/EDA_tools.py\" \"aiml2days/notebooks/modeling_tools.py\" . \n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"aiml2days/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build a spam detector\n",
    "\n",
    "### Task \n",
    "\n",
    "We want to build a Spam detector which, given examples of spam emails (e.g. flagged by users) and examples of regular (non-spam, also called \"ham\") emails, learns how to flag new unseen emails as spam or non-spam.\n",
    "\n",
    "### Data\n",
    "\n",
    "In the previous notebooks we have covered the following steps:\n",
    "* Load the data\n",
    "* Text preprocessing\n",
    "* Feature extraction\n",
    "* Data exploration\n",
    "\n",
    "The different feature sets can be loaded with the `load_feature_space()`-function. The options for the feature sets are:\n",
    "* \"num\": numerical features\n",
    "* \"text\": text features\n",
    "* \"num_text\": numerical and text features combined\n",
    "* \"embedding\": embedding features\n",
    "\n",
    "### In this notebook\n",
    "\n",
    "Our aim is to build a simple spam detector. We will start by exploring the different datasets, before we will build a simple spam detector and evaluate the model. We will use the following steps:\n",
    "* Load the features\n",
    "* Split the data into training and test set\n",
    "* Train a simple spam detector\n",
    "* Evaluate the model\n",
    "* Analyze misclassified samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and helper functions\n",
    "%run data_prep_tools.py\n",
    "%run EDA_tools.py\n",
    "%run modeling_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data sets\n",
    "num_features_df = load_feature_space(\"num\", no_labels=False)\n",
    "text_features_df = load_feature_space(\"text\", no_labels=False)\n",
    "num_text_features_df = load_feature_space(\"num_text\", no_labels=False)\n",
    "embeddings_df = load_feature_space(\"embedding\", no_labels=False)\n",
    "labels = load_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of samples per class in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_frequency(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "Suppose you applied a **very naive approach** to the spam detection problem **that uses none of the features**: _You just either classify all emails as \"spam\" or as \"non-spam\"._\n",
    "\n",
    "__Q1.__ How many emails would be classified correctly in each case?  \n",
    "\n",
    "__Q2.__ Which approach would be more successful?\n",
    "\n",
    "This naive kind of approach is useful to establish a **baseline** for the performance of our more complex classifiers.  \n",
    "In this notebook we will build various spam detectors and compare their performance to this baseline.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give your answer here:\n",
    "\n",
    "1.    \n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n",
    "\n",
    "We are now ready to build our machine learning model for detecting spams. \n",
    "\n",
    "Throughout this notebook we will use a **Logistic Regression classifier**. Here is why:\n",
    "- It is a simple and efficient model for binary classification tasks. \n",
    "- It is a good baseline for more complex models. \n",
    "- It is fast to train and thus allows us to quickly iterate on our model and try out different settings.\n",
    "- It is also easy to interpret and allows us to explore where our model makes mistakes.\n",
    "\n",
    "Below you will\n",
    "- build a first simple model.\n",
    "- tune the main hyperparameter `C` for the model using a cross-validated grid search.\n",
    "- explore different feature sets and see how they affect the performance of the model.\n",
    "- explore the effect of different evaluation metrics.\n",
    "- Explore misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first trial\n",
    "\n",
    "As a first trial, we will use the `num` feature set with a simpl. The accuracy is defined as the number of correct predictions divided by the total number of predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(num_features_df)\n",
    "\n",
    "# Fit model on the train data\n",
    "model = fit_model(df_train, C=1)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "Let's explore the plot above. \n",
    "\n",
    "__Q1.__ Which numbers tell use the correct predictions for each class?\n",
    "\n",
    "__Q2.__ Which numbers tell use the failed predictions for each class?\n",
    "\n",
    "__Q3.__ What class faired better?\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give your answer here:\n",
    "\n",
    "1.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report provides us with different 4 metrics to evaluate the performance of our model: 3 metrics for each class and the overall accuracy. \n",
    "\n",
    "The **accuracy** is the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "The **precision** is the number of true positives divided by the number of true positives plus the number of false positives.\n",
    "\n",
    "The **recall** is the number of true positives divided by the number of true positives plus the number of false negatives.\n",
    "\n",
    "The **f1-score** is the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for test set\n",
    "classification_report_(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can consider changing the `C` parameter in the `fit_model` function to see how it affects the model performance. But this is seen as bad practice be cause you are tuning the mmodel to the test set, which you also use for evaluation.\n",
    "\n",
    "Instead let's use a more appropriate approach below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more systematic approach to fine tuning\n",
    "\n",
    "We will use 5-fold cross-validation. So the validation sets are automatically created internally. The test set will be used to `evaluate` the performance of our model. The process will automatically choose the best model for us. We also collect all the results from the cross-validation so we can plot them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modeling_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(num_features_df)\n",
    "# text_features_df takes 45mins\n",
    "\n",
    "# Fit model on the train data\n",
    "model, cv_results = fit_log_reg_model(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking overfitting\n",
    "\n",
    "Below we plot the results of the cross-validation. The x-axis shows the different values of `C` we tried, and the y-axis shows the metric of the model evaluated on the training set (blue) and on the validation set (orange). The red cross shows the value of the best `C` we found.\n",
    "\n",
    "We are interested in the gap between the training and validation curves. If the gap is small, it means that our model is not overfitting and generalizes well to unseen data. If the gap is large, it means that our model is overfitting. This indicates that the model has learned irrelevant information like noise that does not reflect the general pattern. In such a case we need to find ways to adjust the model to reduce the gap and improve the performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cv_results(cv_results, show_table=False, plot_confidence=False, plot_fit_time=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "Let's explore the plot above. \n",
    "\n",
    "__Q1.__ Do we observve overfitting i.e. a large gap between the training and validation curves?\n",
    "\n",
    "__Q2.__ What happens when C is very small and when it is very large?\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give your answer here:\n",
    "\n",
    "1.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for test set\n",
    "classification_report_(df_test, model)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more insights into the model\n",
    "\n",
    "In the Logistic Regression the coefficients tell us how much each feature contributes to the overall prediction. The larger the absolute value of a coefficient, the more important the corresponding feature is for the model. \n",
    "\n",
    "For the numerical features and the text features we always have positive feature values. Thus positive coefficients contribute more to the `spam` class while negative coefficients contribute more to the `non-spam` class.\n",
    "\n",
    "For the embedding features we have both positive and negative feature values. Thus we need to look at the contributions (feature values times coefficients) to judge the impact on the overall prediction. This will help us understand the model's behavior better and identify which features are driving the predictions for particular samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coefficients(model, df_train, n_top_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sure was the model of its predictions?\n",
    "The Logistic Regression model can return the probabilities of each class for each sample. The probabilities are between 0 and 1, and the sum of the probabilities for each sample is 1. This allows us to assess the confidence of the model's predictions.\n",
    "\n",
    "Below we plot the probabilities of the spam class and colour them by their actual class. Low probabilities (close to 0) indicate that the model is very sure that the sample is not spam, while high probabilities (close to 1) indicate that the model is very sure that the sample is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_certainties(df_test, model, log_scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "Let's explore the plot above. \n",
    "\n",
    "__Q1.__ When the model misclassified a sample, is it usually very sure of its prediction, or kind of doubtful?\n",
    "\n",
    "Careful with interpretation when the top plot using a log-scale. This means that the values are not evenly spaced. You can change the setting.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give your answer here:\n",
    "\n",
    "1.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis :: Where does our model fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now analyze the misclassified mails in order to get some insights on where the model failed to make correct predictions. The *error_analysis* function below will show us the top features responsible for the model making a decision of prediction whether the mail is spam or non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modeling_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(df_test, model, doc_nbr=10, n_top_coeff=5, color_by_coeff_sign=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "\n",
    "# NOW IT'S YOUR TURN\n",
    "\n",
    "We have copied the above code blocks again below. You can use them to build your own spam detector now. \n",
    "\n",
    "There are a number of things you can adapt:\n",
    "\n",
    "### Change the feature space\n",
    "\n",
    "We have loaded 4 feature spaces at the start of the notebook. Simply replace `num_features_df` with `text_features`, `num_text_features`, or `embedding_features` in the code below to use a different feature space.\n",
    "\n",
    "Warning: The feature spaces using text features are quite slow (45 mins) and will take quite a while to run the fine-tuning with cross-validation.  \n",
    "The pre-computed output of grid search with cross-validation can be loaded with the following code `pd.read_csv(\"text_log_reg_cv_results.csv\")`\n",
    "    \n",
    "You can retrain the model using the `fit_model`-function with `C` set to the best `C`-value from `cv_results`.\n",
    "    \n",
    "### Change the metric used for fine-tuning\n",
    "\n",
    "You can change the scoring function inside `fit_log_reg_model(df_train)`.  \n",
    "\n",
    "The current default value is `None`, which means that the model will use the default scoring function for the Logistic Regression model. This is the accuracy score. \n",
    "\n",
    "However, you can also change the scoring function to `\"precision\"`, `\"recall\"`, or `\"f1\"` and check how the results change. More options are given [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
    "\n",
    "What happens to the confusion matrix as you vary the metric?\n",
    "\n",
    "### Explore other settings\n",
    "\n",
    "You will likely have to change some of the other parameters in the visualisations, etc. to make them more interpretable.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make things easier:\n",
    "# Change your settings here and then run the cell below\n",
    "\n",
    "feature_space = num_features_df\n",
    "# options are: num_features_df, text_features_df, num_text_features_df, embeddings_df\n",
    "\n",
    "\n",
    "C = 1\n",
    "# options to try are: 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000\n",
    "\n",
    "\n",
    "scoring = None\n",
    "# options include: 'accuracy', 'f1', 'precision', 'recall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(feature_space)\n",
    "\n",
    "# Fit model on the train data\n",
    "model = fit_model(df_train, C=C)\n",
    "\n",
    "# Print classification report for test set\n",
    "classification_report_(df_test, model)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(feature_space)\n",
    "# text_features_df takes 45mins\n",
    "\n",
    "# Fit model on the train data\n",
    "model, cv_results = fit_log_reg_model(df_train, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cv_results(cv_results, show_table=False, plot_confidence=False, plot_fit_time=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for test set\n",
    "classification_report_(df_test, model)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more insights into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coefficients(model, df_train, n_top_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sure was the model of its predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_certainties(df_test, model, log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis :: Where does our model fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(df_test, model, doc_nbr=10, n_top_coeff=15, color_by_coeff_sign=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adsml2024]",
   "language": "python",
   "name": "conda-env-adsml2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
